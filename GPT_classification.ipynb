{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/code/data/train/subtask_1/en/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ33u29jCuOK",
        "outputId": "26c78c37-0eda-44ea-9987-80a0f81d63a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1HP9wLDDDLG",
        "outputId": "66e2783e-8405-4edd-c382-7762a052d374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.28.1-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import transformers\n",
        "import torch\n",
        "import openai"
      ],
      "metadata": {
        "id": "uoWdw3nFCwgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_human = pd.read_csv(\"human.csv\")\n",
        "df_bloom1b7 = pd.read_csv(\"bloom1b7.csv\")\n",
        "df_bloom3b = pd.read_csv(\"bloom3b.csv\")\n",
        "df_bloom7b1 = pd.read_csv(\"bloom7b1.csv\")\n",
        "df_gpt = pd.read_csv(\"gpt.csv\")"
      ],
      "metadata": {
        "id": "JHhQ0AdLCx0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('api-key')"
      ],
      "metadata": {
        "id": "vszZrzK2DHsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_texts(df, prompt_template, output_path):\n",
        "    openai.api_key = api_key\n",
        "\n",
        "    # Initialize a list to store the results\n",
        "    classification_results = []\n",
        "\n",
        "    # Loop through each text in the dataset\n",
        "    for index, row in df.iterrows():\n",
        "        text = row['text']\n",
        "        label = row['label']\n",
        "        domain = row['domain']\n",
        "        text_id = row['id']\n",
        "        text_to_classify = row['text']\n",
        "\n",
        "        # Create prompt message based on template\n",
        "        prompt = prompt_template.format(text_to_classify=text_to_classify)\n",
        "\n",
        "        completion = openai.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a classification assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Extract the response\n",
        "        generated_text = completion.choices[0].message.content\n",
        "\n",
        "        classification_results.append({'id': text_id, 'text': text, 'generated_label': generated_text, 'label': label, 'domain': domain})\n",
        "\n",
        "    # Convert the results into a DataFrame\n",
        "    classification_df = pd.DataFrame(classification_results)\n",
        "    classification_df.to_csv(output_path, index=False)"
      ],
      "metadata": {
        "id": "hb60xoWdCzzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Rate the likelihood of this text being generated by a large language model on a scale of 1 (very unlikely) to 5 (very likely). Please provide your conclusion: was this text likely generated by a machine or a human?\n",
        "Text: {text_to_classify}\n",
        "Response format: Likelihood, Machine or Human\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NcpR5-5CsmLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify_texts(df_human, prompt, \"classification_results_gpt_human.csv\")"
      ],
      "metadata": {
        "id": "Cxd0jB06stzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify_texts(df_bloom1b7, prompt, \"classification_results_gpt_1b7.csv\")"
      ],
      "metadata": {
        "id": "jlnQ6QodswHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify_texts(df_bloom3b, prompt, \"classification_results_gpt_3b.csv\")"
      ],
      "metadata": {
        "id": "roxYFoNCszcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify_texts(df_gpt, prompt, \"classification_results_gpt_gpt.csv\")"
      ],
      "metadata": {
        "id": "D0PFRbpTs2X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "def clean_generated_text(generated_text):\n",
        "    if 'human' in generated_text.lower():\n",
        "        return 'human'\n",
        "    elif 'machine' in generated_text.lower():\n",
        "        return 'generated'\n",
        "    #elif 'generated' in generated_text.lower():\n",
        "    #    return 'machine'\n",
        "    elif 'AI' in generated_text:\n",
        "        return 'generated'\n",
        "    elif 'large language model' in generated_text.lower():\n",
        "        return 'generated'\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def results_evaluation(file_path, correct_answer):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Extract labels from the text\n",
        "    df['generated_label'] = df['generated_label'].apply(clean_generated_text)\n",
        "\n",
        "    # Drop rows with missing or incorrect labels\n",
        "    df.dropna(subset=['generated_label'], inplace=True)\n",
        "    df = df[df['generated_label'].isin(['generated', 'human'])]\n",
        "\n",
        "    df['correct_answer'] = correct_answer\n",
        "\n",
        "    report = classification_report(df['correct_answer'], df['generated_label'], zero_division=1)\n",
        "\n",
        "    micro_f1 = f1_score(df['correct_answer'], df['generated_label'], average='micro')\n",
        "\n",
        "    print(df['generated_label'].value_counts())\n",
        "\n",
        "    return micro_f1, report"
      ],
      "metadata": {
        "id": "hE7PBBKDhqNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Results"
      ],
      "metadata": {
        "id": "rz8Kw2md_1qT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report, f1 = results_evaluation(\"classification_results_gpt_human.csv\", \"human\" )\n",
        "print(report)\n",
        "\n",
        "print(f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_nUujl59Hzj",
        "outputId": "b2b3f80b-d685-4174-956b-c9dc231f6f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated_label\n",
            "human        551\n",
            "generated    449\n",
            "Name: count, dtype: int64\n",
            "0.551\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   generated       0.00      1.00      0.00         0\n",
            "       human       1.00      0.55      0.71      1000\n",
            "\n",
            "    accuracy                           0.55      1000\n",
            "   macro avg       0.50      0.78      0.36      1000\n",
            "weighted avg       1.00      0.55      0.71      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report, f1 = results_evaluation(\"classification_results_gpt_1b7.csv\", \"generated\" )\n",
        "print(report)\n",
        "\n",
        "print(f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RNeoT4R9a6a",
        "outputId": "305a6e71-0afc-48a8-e25d-240020fa2509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated_label\n",
            "generated    663\n",
            "human        337\n",
            "Name: count, dtype: int64\n",
            "0.663\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   generated       1.00      0.66      0.80      1000\n",
            "       human       0.00      1.00      0.00         0\n",
            "\n",
            "    accuracy                           0.66      1000\n",
            "   macro avg       0.50      0.83      0.40      1000\n",
            "weighted avg       1.00      0.66      0.80      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report, f1 = results_evaluation(\"classification_results_gpt_3b.csv\", \"generated\" )\n",
        "print(report)\n",
        "\n",
        "print(f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZR_G2Lg9e2C",
        "outputId": "ae6e8db7-9bc3-4d11-d419-7e9ee673c0e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated_label\n",
            "generated    568\n",
            "human        432\n",
            "Name: count, dtype: int64\n",
            "0.568\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   generated       1.00      0.57      0.72      1000\n",
            "       human       0.00      1.00      0.00         0\n",
            "\n",
            "    accuracy                           0.57      1000\n",
            "   macro avg       0.50      0.78      0.36      1000\n",
            "weighted avg       1.00      0.57      0.72      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report, f1 = results_evaluation(\"classification_results_gpt_gpt.csv\", \"generated\" )\n",
        "print(report)\n",
        "\n",
        "print(f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMPkR0_89kjX",
        "outputId": "109570d9-6e58-41a1-8bc9-73dadb6352ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated_label\n",
            "human        663\n",
            "generated    337\n",
            "Name: count, dtype: int64\n",
            "0.337\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   generated       1.00      0.34      0.50      1000\n",
            "       human       0.00      1.00      0.00         0\n",
            "\n",
            "    accuracy                           0.34      1000\n",
            "   macro avg       0.50      0.67      0.25      1000\n",
            "weighted avg       1.00      0.34      0.50      1000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}